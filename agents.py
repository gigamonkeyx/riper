"""
Specialist Agents Module for RIPER-Î© System
Ollama-based agents optimized for RTX 3080 GPU tasks.

Agents:
- OllamaSpecialist: Base class for Ollama model integration
- FitnessScorer: Evolutionary fitness evaluation specialist
- TTSHandler: Text-to-speech processing with Bark integration
- SwarmCoordinator: CrewAI-inspired agent duplication and coordination
"""

import logging
import time
import json
import subprocess
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass
from abc import ABC, abstractmethod
from openrouter_client import get_openrouter_client
from protocol import builder_output_fitness, check_builder_bias

# Optional imports with fallbacks
try:
    import torch

    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import requests

    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

logger = logging.getLogger(__name__)


def check_gpu_memory() -> Dict[str, Any]:
    """Check GPU memory usage via nvidia-smi"""
    try:
        result = subprocess.run(
            [
                "nvidia-smi",
                "--query-gpu=memory.used,memory.total",
                "--format=csv,noheader,nounits",
            ],
            capture_output=True,
            text=True,
            timeout=5,
        )
        if result.returncode == 0:
            used, total = map(int, result.stdout.strip().split(", "))
            usage_pct = (used / total) * 100
            return {
                "used_mb": used,
                "total_mb": total,
                "usage_percent": usage_pct,
                "available_mb": total - used,
                "over_8gb": used > 8192,
            }
    except Exception as e:
        logger.warning(f"GPU memory check failed: {e}")

    return {"error": "nvidia-smi unavailable"}


@dataclass
class TaskResult:
    """Standard result structure for agent tasks"""

    success: bool
    data: Any
    execution_time: float
    gpu_utilized: bool
    error_message: Optional[str] = None


class OllamaSpecialist(ABC):
    """
    Base class for Ollama-based specialist agents
    Optimized for RTX 3080 GPU performance (7-15 tok/sec target)
    """

    def __init__(self, model_name: str = "qwen3:8b", gpu_enabled: bool = True):
        self.model_name = model_name
        self.gpu_enabled = gpu_enabled and TORCH_AVAILABLE
        self.base_url = "http://localhost:11434"  # Default Ollama endpoint
        self.performance_target = {"min_tok_sec": 7, "max_tok_sec": 15}

        # Verify Ollama availability
        self.ollama_available = self._check_ollama_availability()

        logger.info(
            f"OllamaSpecialist initialized: model={model_name}, GPU={self.gpu_enabled}"
        )

    def _check_ollama_availability(self) -> bool:
        """Check if Ollama service is available"""
        try:
            if REQUESTS_AVAILABLE:
                response = requests.get(f"{self.base_url}/api/tags", timeout=5)
                return response.status_code == 200
            else:
                # Fallback: check via subprocess
                result = subprocess.run(
                    ["ollama", "list"], capture_output=True, timeout=5
                )
                return result.returncode == 0
        except Exception as e:
            logger.warning(f"Ollama availability check failed: {e}")
            return False

    def _call_ollama(
        self, prompt: str, system_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """Make API call to Ollama with CLI fallback as primary"""
        if not self.ollama_available:
            return {"error": "Ollama not available", "response": ""}

        # Check GPU memory before call
        gpu_info = check_gpu_memory()
        if gpu_info.get("over_8gb", False):
            logger.warning(f"âš ï¸ High VRAM usage: {gpu_info.get('used_mb', 0)}MB")

        # Use CLI fallback as primary method due to API instability
        logger.info("Using CLI fallback as primary method")
        return self._ollama_subprocess_call({"prompt": prompt, "system": system_prompt})

        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "num_gpu": 1 if self.gpu_enabled else 0,
                "num_thread": 8,  # RTX 3080 optimization
                "temperature": 0.7,
            },
        }

        if system_prompt:
            payload["system"] = system_prompt

        try:
            start_time = time.time()

            if REQUESTS_AVAILABLE:
                response = requests.post(
                    f"{self.base_url}/api/generate", json=payload, timeout=300.0
                )
                result = response.json()
            else:
                # Fallback: use subprocess
                result = self._ollama_subprocess_call(payload)

            execution_time = time.time() - start_time

            # Estimate tokens per second (rough approximation)
            response_text = result.get("response", "")
            estimated_tokens = len(response_text.split())
            tok_sec = estimated_tokens / execution_time if execution_time > 0 else 0

            logger.debug(f"Ollama call completed: {tok_sec:.1f} tok/sec (target: 7-15)")

            return {
                "response": response_text,
                "execution_time": execution_time,
                "tokens_per_second": tok_sec,
                "gpu_used": self.gpu_enabled,
                "success": True,
            }

        except Exception as e:
            logger.error(f"Ollama API call failed: {e}")
            # CLI fallback for timeouts >30s
            if "timeout" in str(e).lower():
                logger.info("Attempting CLI fallback...")
                return self._ollama_subprocess_call(payload)
            return {"error": str(e), "response": "", "success": False}

    def _ollama_subprocess_call(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """Fallback Ollama call using subprocess"""
        try:
            cmd = ["ollama", "run", self.model_name, payload["prompt"]]
            start_time = time.time()
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            execution_time = time.time() - start_time

            if result.returncode == 0:
                response_text = result.stdout.strip()
                return {
                    "response": response_text,
                    "execution_time": execution_time,
                    "success": True,
                    "method": "cli_fallback",
                }
            else:
                return {"error": result.stderr, "response": "", "success": False}
        except Exception as e:
            return {"error": str(e), "response": "", "success": False}

    @abstractmethod
    def process_task(self, task_data: Any, **kwargs) -> TaskResult:
        """Process specialist task - to be implemented by subclasses"""
        pass


class FitnessScorer(OllamaSpecialist):
    """
    Specialist agent for evolutionary fitness evaluation
    Uses Qwen3-Coder for intelligent fitness scoring
    Enhanced with v2.6 bias detection and RL rewards
    """

    def __init__(self):
        super().__init__(
            model_name="deepseek-coder:1.3b"
        )  # Use smaller model for testing
        self.fitness_history: List[Dict[str, Any]] = []
        self.openrouter_client = get_openrouter_client()

    def process_task(self, task_data: Any, **kwargs) -> TaskResult:
        """Evaluate fitness for evolutionary algorithms"""
        start_time = time.time()

        generation = kwargs.get("generation", 0)
        current_fitness = kwargs.get("current_fitness", 0.0)

        # Construct fitness evaluation prompt
        system_prompt = """You are an expert evolutionary algorithm fitness evaluator. 
        Analyze the provided data and suggest improvements for neural network evolution.
        Focus on RTX 3080 GPU optimization and >70% fitness threshold achievement."""

        prompt = f"""
        Evaluate evolutionary fitness for generation {generation}:
        Current fitness: {current_fitness:.4f}
        Target threshold: 0.70 (70%)
        
        Task data: {task_data}
        
        Provide:
        1. Fitness assessment (0.0-1.0 scale)
        2. Improvement suggestions
        3. GPU optimization recommendations
        4. Next generation strategy
        
        Format as JSON.
        """

        try:
            # Hybrid fitness evaluation: OpenRouter + Ollama
            fitness_evaluations = {}

            # 1. OpenRouter Qwen3 evaluation (cloud-based intelligence)
            try:
                qwen3_response = self.openrouter_client.qwen3_fitness_analysis(
                    fitness_data={
                        "generation": generation,
                        "current_fitness": current_fitness,
                        "task_data": task_data,
                        "target_threshold": 0.70,
                    },
                    generation=generation,
                )

                if qwen3_response.success:
                    fitness_evaluations["qwen3"] = {
                        "response": qwen3_response.content,
                        "execution_time": qwen3_response.execution_time,
                        "success": True,
                    }
                else:
                    fitness_evaluations["qwen3"] = {
                        "error": qwen3_response.error_message,
                        "success": False,
                    }
            except Exception as e:
                fitness_evaluations["qwen3"] = {"error": str(e), "success": False}

            # 2. Ollama local evaluation (local GPU intelligence)
            try:
                ollama_result = self._call_ollama(prompt, system_prompt)
                fitness_evaluations["ollama"] = {
                    "response": ollama_result.get("response", ""),
                    "success": ollama_result.get("success", False),
                }
            except Exception as e:
                fitness_evaluations["ollama"] = {"error": str(e), "success": False}

            # 3. Combine evaluations for final fitness score
            fitness_score = self._combine_fitness_evaluations(
                fitness_evaluations, current_fitness
            )

            # Prepare response text for history
            response_text = self._format_hybrid_response(fitness_evaluations)

            # Store in history
            evaluation = {
                "generation": generation,
                "fitness_score": fitness_score,
                "hybrid_response": response_text,
                "evaluations": fitness_evaluations,
                "timestamp": time.time(),
            }
            self.fitness_history.append(evaluation)

            execution_time = time.time() - start_time

            # v2.6: Add bias detection to fitness evaluation
            bias_analysis = self.detect_output_bias(response_text, str(fitness_evaluations))

            return TaskResult(
                success=True,
                data={
                    "fitness_score": fitness_score,
                    "evaluation": evaluation,
                    "improvement_suggestions": response_text,
                    "bias_analysis": bias_analysis,  # v2.6 addition
                },
                execution_time=execution_time,
                gpu_utilized=self.gpu_enabled,
            )

        except Exception as e:
            logger.error(f"Fitness scoring failed: {e}")
            execution_time = time.time() - start_time

            return TaskResult(
                success=False,
                data={"fitness_score": current_fitness},
                execution_time=execution_time,
                gpu_utilized=False,
                error_message=str(e),
            )

    def detect_output_bias(self, output_text: str, log_text: str = "") -> Dict[str, Any]:
        """
        v2.6: Detect bias in fitness evaluation outputs
        Penalizes false positives <0.70 as per RIPER-Î© protocol
        """
        try:
            bias_analysis = check_builder_bias(output_text, log_text)

            if bias_analysis['bias_detected']:
                logger.warning(f"Bias detected in fitness output: {bias_analysis['fitness_score']:.3f}")
                for detail in bias_analysis['details']:
                    logger.warning(f"  - {detail}")

            return bias_analysis

        except Exception as e:
            logger.error(f"Bias detection failed: {e}")
            return {
                'bias_detected': False,
                'fitness_score': 1.0,
                'details': [],
                'threshold_met': True,
                'error': str(e)
            }

    def _combine_fitness_evaluations(
        self, evaluations: Dict[str, Any], current_fitness: float
    ) -> float:
        """Combine OpenRouter and Ollama fitness evaluations"""
        scores = []

        # Extract Qwen3 score
        if evaluations.get("qwen3", {}).get("success"):
            qwen3_response = evaluations["qwen3"]["response"]
            try:
                if isinstance(qwen3_response, str) and "{" in qwen3_response:
                    analysis = json.loads(qwen3_response)
                    qwen3_score = analysis.get("fitness_assessment", current_fitness)
                    scores.append(("qwen3", float(qwen3_score), 0.6))  # 60% weight
                else:
                    qwen3_score = self._extract_fitness_score(qwen3_response)
                    scores.append(("qwen3", qwen3_score, 0.6))
            except (json.JSONDecodeError, ValueError):
                pass

        # Extract Ollama score
        if evaluations.get("ollama", {}).get("success"):
            ollama_response = evaluations["ollama"]["response"]
            try:
                ollama_score = self._extract_fitness_score(ollama_response)
                scores.append(("ollama", ollama_score, 0.4))  # 40% weight
            except ValueError:
                pass

        # Calculate weighted average or fallback
        if scores:
            weighted_sum = sum(score * weight for _, score, weight in scores)
            total_weight = sum(weight for _, _, weight in scores)
            return min(weighted_sum / total_weight, 1.0)
        else:
            # Fallback to gradual improvement
            return min(current_fitness + 0.05, 1.0)

    def _format_hybrid_response(self, evaluations: Dict[str, Any]) -> str:
        """Format hybrid evaluation response for history"""
        response_parts = []

        if evaluations.get("qwen3", {}).get("success"):
            response_parts.append(f"Qwen3: {evaluations['qwen3']['response']}")
        elif "qwen3" in evaluations:
            response_parts.append(
                f"Qwen3 Error: {evaluations['qwen3'].get('error', 'Unknown')}"
            )

        if evaluations.get("ollama", {}).get("success"):
            response_parts.append(f"Ollama: {evaluations['ollama']['response']}")
        elif "ollama" in evaluations:
            response_parts.append(
                f"Ollama Error: {evaluations['ollama'].get('error', 'Unknown')}"
            )

        return (
            " | ".join(response_parts) if response_parts else "No evaluations available"
        )

    def _extract_fitness_score(self, response_text: str) -> float:
        """Extract fitness score from Ollama response"""
        try:
            # Try JSON parsing first
            if "{" in response_text and "}" in response_text:
                json_start = response_text.find("{")
                json_end = response_text.rfind("}") + 1
                json_text = response_text[json_start:json_end]
                data = json.loads(json_text)

                # Look for fitness-related keys
                for key in ["fitness", "fitness_score", "score", "assessment"]:
                    if key in data:
                        return float(data[key])

            # Fallback: search for numeric patterns
            import re

            numbers = re.findall(r"0\.\d+|1\.0", response_text)
            if numbers:
                return float(numbers[0])

            # Default fallback
            return 0.5

        except Exception:
            return 0.5

    def evaluate_generation(
        self, generation: int, current_fitness: float
    ) -> Dict[str, Any]:
        """Evaluate a specific generation (convenience method)"""
        task_data = {
            "generation": generation,
            "current_fitness": current_fitness,
            "target_threshold": 0.70,
        }

        result = self.process_task(
            task_data, generation=generation, current_fitness=current_fitness
        )
        return result.data


class TTSHandler(OllamaSpecialist):
    """
    Text-to-Speech handler with Bark integration
    Processes audio generation tasks via Ollama coordination
    """

    def __init__(self):
        super().__init__(model_name="qwen3:8b")  # Use available qwen3:8b model

        # Enforce D: drive cache compliance BEFORE any imports
        self._setup_d_drive_cache()

        # Setup Bark compatibility
        self._setup_bark_compatibility()

        self.bark_available = self._check_bark_availability()

    def _setup_d_drive_cache(self):
        """EMERGENCY: Force D: drive cache compliance for all TTS operations"""
        import os
        import tempfile

        # CRITICAL: Force ALL cache directories to D: drive with Windows paths
        cache_vars = {
            "HF_HOME": "D:\\huggingface_cache",
            "TRANSFORMERS_CACHE": "D:\\transformers_cache",
            "HF_DATASETS_CACHE": "D:\\datasets_cache",
            "TORCH_HOME": "D:\\torch_cache",
            "XDG_CACHE_HOME": "D:\\cache",
            "TMPDIR": "D:\\temp",
            "TEMP": "D:\\temp",
            "TMP": "D:\\temp",
            "PYTORCH_TRANSFORMERS_CACHE": "D:\\transformers_cache",
            "PYTORCH_PRETRAINED_BERT_CACHE": "D:\\transformers_cache",
            "HUGGINGFACE_HUB_CACHE": "D:\\huggingface_cache",
            "SUNO_OFFLOAD_CPU": "True",
            "SUNO_USE_SMALL_MODELS": "True",
            "BARK_CACHE_DIR": "D:\\bark_cache",
        }

        # FORCE override ALL environment variables
        for var, path in cache_vars.items():
            os.environ[var] = path
            logger.info(f"FORCED {var} = {path}")

        # Set Python tempfile directory
        tempfile.tempdir = "D:\\temp"

        # Create directories if they don't exist
        cache_dirs = [
            "D:\\huggingface_cache",
            "D:\\transformers_cache",
            "D:\\datasets_cache",
            "D:\\torch_cache",
            "D:\\cache",
            "D:\\temp",
            "D:\\bark_cache",
        ]

        for cache_dir in cache_dirs:
            os.makedirs(cache_dir, exist_ok=True)
            logger.info(f"Created cache directory: {cache_dir}")

        # CRITICAL: Override torch hub directory
        try:
            import torch

            torch.hub.set_dir("D:\\torch_cache")
            logger.info("FORCED torch.hub directory to D: drive")
        except Exception as e:
            logger.warning(f"Could not set torch.hub directory: {e}")

        logger.info("ðŸš¨ EMERGENCY D: DRIVE CACHE COMPLIANCE ENFORCED")

    def _setup_bark_compatibility(self):
        """Setup Bark TTS compatibility with PyTorch 2.6+"""
        try:
            from bark_compatibility import setup_bark_compatibility

            setup_bark_compatibility()
            logger.info("âœ… Bark compatibility setup completed")
        except ImportError:
            logger.warning("Bark compatibility module not found - using fallback")
        except Exception as e:
            logger.warning(f"Bark compatibility setup failed: {e}")

    def _check_bark_availability(self) -> bool:
        """Check if Bark TTS is available"""
        try:
            # Comprehensive PyTorch 2.6+ compatibility setup
            import torch
            import warnings

            # Suppress deprecation warnings
            warnings.filterwarnings("ignore", category=FutureWarning)
            warnings.filterwarnings("ignore", category=UserWarning)

            # Monkey patch torch.load to handle Bark models
            original_load = torch.load

            def bark_compatible_load(*args, **kwargs):
                # Force weights_only=False for Bark models (trusted source)
                kwargs["weights_only"] = False
                return original_load(*args, **kwargs)

            torch.load = bark_compatible_load

            # Add comprehensive safe globals
            torch.serialization.add_safe_globals(
                [
                    "numpy.core.multiarray.scalar",
                    "numpy.dtype",
                    "numpy.ndarray",
                    "collections.OrderedDict",
                    "torch._utils._rebuild_tensor_v2",
                    "torch.nn.parameter.Parameter",
                    "torch.Tensor",
                ]
            )

            # Check if bark can be imported
            import bark

            # Restore original torch.load after successful import
            torch.load = original_load

            logger.info(
                "Bark TTS available with comprehensive PyTorch 2.6+ compatibility"
            )
            return True

        except ImportError:
            logger.info("Bark TTS not available - using Ollama text processing only")
            return False
        except Exception as e:
            logger.warning(f"Bark TTS check failed: {e}")
            return False

    def process_task(self, task_data: Any, **kwargs) -> TaskResult:
        """Process TTS task with Ollama coordination"""
        start_time = time.time()

        text_input = kwargs.get("text", str(task_data))
        voice_preset = kwargs.get("voice_preset", "v2/en_speaker_6")

        # Use Ollama to optimize text for TTS
        system_prompt = """You are a text-to-speech optimization specialist.
        Prepare text for high-quality audio generation, considering pronunciation,
        pacing, and clarity. Optimize for Bark TTS model requirements."""

        prompt = f"""
        Optimize the following text for TTS generation:
        
        Text: {text_input}
        Voice preset: {voice_preset}
        
        Provide:
        1. Optimized text with proper punctuation and pacing
        2. Pronunciation notes for difficult words
        3. Suggested voice settings
        4. Audio quality recommendations
        
        Return optimized text and metadata.
        """

        # Get Ollama optimization
        ollama_result = self._call_ollama(prompt, system_prompt)
        optimized_text = ollama_result.get("response", text_input)

        # Generate audio if Bark is available
        audio_result = None
        if self.bark_available:
            audio_result = self._generate_bark_audio(optimized_text, voice_preset)

        execution_time = time.time() - start_time

        return TaskResult(
            success=True,
            data={
                "original_text": text_input,
                "optimized_text": optimized_text,
                "audio_generated": audio_result is not None,
                "audio_data": audio_result,
                "ollama_optimization": ollama_result,
            },
            execution_time=execution_time,
            gpu_utilized=self.gpu_enabled,
        )

    def _generate_bark_audio(
        self, text: str, voice_preset: str
    ) -> Optional[Dict[str, Any]]:
        """Generate audio using Bark TTS with comprehensive compatibility fixes"""
        if not self.bark_available:
            return None

        try:
            import torch
            import warnings
            import numpy as np

            # Suppress all warnings during Bark operations
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")

                # Comprehensive PyTorch compatibility setup
                original_load = torch.load

                def bark_load_fix(*args, **kwargs):
                    # Force compatibility settings for Bark
                    kwargs["weights_only"] = False
                    kwargs["map_location"] = "cpu"  # Load to CPU first
                    return original_load(*args, **kwargs)

                # Temporarily replace torch.load
                torch.load = bark_load_fix

                try:
                    # Import Bark components
                    from bark import SAMPLE_RATE, generate_audio, preload_models
                    from bark.generation import SUPPORTED_LANGS

                    logger.info("Bark modules imported successfully")

                    # Preload models with error handling
                    try:
                        preload_models()
                        logger.info("Bark models preloaded successfully")
                    except Exception as preload_error:
                        logger.warning(f"Model preload warning: {preload_error}")
                        # Continue anyway, models may load on-demand

                    # Generate audio with error handling
                    try:
                        # Limit text length to prevent memory issues
                        if len(text) > 200:
                            text = text[:200] + "..."

                        # Use a safe voice preset
                        safe_voice = voice_preset if voice_preset else "v2/en_speaker_6"

                        logger.info(f"Generating audio for text: '{text[:50]}...'")
                        audio_array = generate_audio(text, history_prompt=safe_voice)

                        # Validate audio output
                        if audio_array is None or len(audio_array) == 0:
                            raise ValueError("Generated audio is empty")

                        # Convert to numpy array if needed
                        if not isinstance(audio_array, np.ndarray):
                            audio_array = np.array(audio_array)

                        logger.info(
                            f"Audio generated successfully: {len(audio_array)} samples"
                        )

                        return {
                            "audio_array": audio_array.tolist(),
                            "sample_rate": SAMPLE_RATE,
                            "duration": len(audio_array) / SAMPLE_RATE,
                            "voice_preset": safe_voice,
                            "text_length": len(text),
                        }

                    except Exception as gen_error:
                        logger.error(f"Audio generation error: {gen_error}")
                        return None

                finally:
                    # Always restore original torch.load
                    torch.load = original_load

        except ImportError as import_error:
            logger.error(f"Bark import failed: {import_error}")
            return None

        except Exception as e:
            logger.error(f"Bark audio generation failed: {e}")
            return None


class SwarmCoordinator(OllamaSpecialist):
    """
    CrewAI-inspired swarm coordination for agent duplication
    Manages multiple specialist agents for parallel processing
    """

    def __init__(self):
        super().__init__(model_name="deepseek-coder:6.7b")  # Use available coding model
        self.active_agents: Dict[str, OllamaSpecialist] = {}
        self.task_queue: List[Dict[str, Any]] = []

    def process_task(self, task_data: Any, **kwargs) -> TaskResult:
        """Coordinate swarm task processing"""
        start_time = time.time()

        task_type = kwargs.get("task_type", "general")
        parallel_agents = kwargs.get("parallel_agents", 2)

        # Create specialized agents for task
        agents = self._create_task_agents(task_type, parallel_agents)

        # Distribute tasks among agents
        results = []
        for i, agent in enumerate(agents):
            agent_task_data = {
                "subtask_id": i,
                "total_subtasks": len(agents),
                "data": task_data,
            }

            try:
                result = agent.process_task(agent_task_data, **kwargs)
                results.append(result)
            except Exception as e:
                logger.error(f"Agent {i} failed: {e}")
                results.append(
                    TaskResult(
                        success=False,
                        data={},
                        execution_time=0,
                        gpu_utilized=False,
                        error_message=str(e),
                    )
                )

        # Aggregate results
        successful_results = [r for r in results if r.success]
        total_execution_time = time.time() - start_time

        return TaskResult(
            success=len(successful_results) > 0,
            data={
                "swarm_results": [r.data for r in results],
                "success_rate": len(successful_results) / len(results),
                "parallel_agents": len(agents),
                "aggregated_data": self._aggregate_results(successful_results),
            },
            execution_time=total_execution_time,
            gpu_utilized=any(r.gpu_utilized for r in results),
        )

    def _create_task_agents(self, task_type: str, count: int) -> List[OllamaSpecialist]:
        """Create specialized agents for specific task types"""
        agents = []

        for i in range(count):
            if task_type == "fitness":
                agent = FitnessScorer()
            elif task_type == "tts":
                agent = TTSHandler()
            else:
                # Generic Ollama specialist
                agent = GenericSpecialist(f"qwen3:latest")

            agents.append(agent)

        return agents

    def _aggregate_results(self, results: List[TaskResult]) -> Dict[str, Any]:
        """Aggregate results from multiple agents"""
        if not results:
            return {}

        # Simple aggregation strategy
        aggregated = {
            "total_agents": len(results),
            "average_execution_time": sum(r.execution_time for r in results)
            / len(results),
            "gpu_utilization_rate": sum(1 for r in results if r.gpu_utilized)
            / len(results),
        }

        # Merge data from all results
        all_data = {}
        for result in results:
            if isinstance(result.data, dict):
                all_data.update(result.data)

        aggregated["merged_data"] = all_data
        return aggregated


class GenericSpecialist(OllamaSpecialist):
    """Generic Ollama specialist for general tasks"""

    def __init__(self, model_name: str = "qwen3:8b"):
        super().__init__(model_name)

    def process_task(self, task_data: Any, **kwargs) -> TaskResult:
        """Process generic task using Ollama"""
        start_time = time.time()

        prompt = f"Process the following task data: {task_data}"
        system_prompt = kwargs.get("system_prompt", "You are a helpful AI assistant.")

        result = self._call_ollama(prompt, system_prompt)
        execution_time = time.time() - start_time

        return TaskResult(
            success="error" not in result,
            data=result,
            execution_time=execution_time,
            gpu_utilized=self.gpu_enabled,
            error_message=result.get("error"),
        )
